{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c2bd6a-5e07-4187-bd32-b37aec469add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Libraries."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30d0c689-483f-42c5-8048-3ad3d7726f56",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf761ad8-2a7b-4bf4-a825-1519accd25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fe5c5a-b4d8-4417-a288-d52495061fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import requests\n",
    "\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e39556-54ac-4434-9b77-8ee87fa4d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset, Preprocess Data, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2adf477a-9807-4fda-9d38-90846a9c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(load_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from pathlib import Path\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Set paths to the directory's\n",
    "    \"\"\"\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    if not os.path.exists(load_data_path):\n",
    "        os.makedirs(load_data_path)\n",
    "\n",
    "        dataset_file = os.path.join(load_data_path, 'aclImdb_v1.tar.gz')\n",
    "        dataset_dir = os.path.join(load_data_path, 'aclImdb')\n",
    "        train_dir = os.path.join(dataset_dir, 'train')\n",
    "        test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "        \"\"\"\n",
    "        ## Load the data: IMDB movie review sentiment classification.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(dataset_file):\n",
    "\n",
    "            url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "            dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                               untar=True, cache_dir=load_data_path,\n",
    "                                               cache_subdir='')\n",
    "\n",
    "            \"\"\"\n",
    "            ## Remove unused directory\n",
    "            \"\"\"\n",
    "\n",
    "            remove_dir = os.path.join(train_dir, 'unsup')\n",
    "\n",
    "            if os.path.isdir(remove_dir):\n",
    "                shutil.rmtree(remove_dir)\n",
    "\n",
    "    \"\"\"\n",
    "    ## Preprocessing Data\n",
    "    \"\"\"\n",
    "\n",
    "    # Utils.files_preprocessing.preprocess_all_folders()         \n",
    "\n",
    "    batch_size = 32\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            train_dir,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            subset=\"training\",\n",
    "            seed=1337,\n",
    "    )\n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            train_dir,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=1337,\n",
    "    )\n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            test_dir, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    tf.data.Dataset.save(raw_train_ds, f'{load_data_path}/raw_train')\n",
    "    tf.data.Dataset.save(raw_val_ds, f'{load_data_path}/raw_val')\n",
    "    tf.data.Dataset.save(raw_test_ds, f'{load_data_path}/raw_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4357565b-fff1-48c9-87d8-05db9a483e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_op = kfp.components.create_component_from_func(download_data,\n",
    "                                                        output_component_file='load_data_component.yaml',\n",
    "                                                        base_image=\"python:3.8\",\n",
    "                                                        packages_to_install=['tensorflow', 'pathlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676d01f4-ae21-4622-81d9-568486896435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0c370f9-faf9-4250-82a2-223a16b3bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(max_features: int, embedding_type: str, embedding_dim: int, sequence_length: int,\n",
    "                 load_data_path: comp.InputPath(str()),\n",
    "                 vectorize_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    \n",
    "    raw_train_ds = tf.data.Dataset.load(f'{load_data_path}/raw_train')\n",
    "    raw_val_ds = tf.data.Dataset.load(f'{load_data_path}/raw_val')\n",
    "    raw_test_ds = tf.data.Dataset.load(f'{load_data_path}/raw_test')\n",
    "\n",
    "    # Build Vectorization Layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "            standardize=None,\n",
    "            max_tokens=max_features,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length,\n",
    "    )\n",
    "\n",
    "    # Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "    # dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "    # datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "    # Let's make a text-only dataset (no labels):\n",
    "    text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "    # Let's call `adapt`:\n",
    "    vectorization_layer.adapt(text_ds)\n",
    "        \n",
    "    # Vectorize the data.  \n",
    "    def vectorize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        return  vectorization_layer(text), label\n",
    "\n",
    "    train_ds = raw_train_ds.map(vectorize_text)\n",
    "    val_ds = raw_val_ds.map(vectorize_text)\n",
    "    test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "    # Do async prefetching / buffering of the data for best performance on GPU.\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    #creating the vectorize directory\n",
    "    os.makedirs(vectorize_data_path, exist_ok = True)\n",
    "    \n",
    "    tf.data.Dataset.save(train_ds, f'{vectorize_data_path}/train')\n",
    "    tf.data.Dataset.save(val_ds, f'{vectorize_data_path}/val')\n",
    "    tf.data.Dataset.save(test_ds, f'{vectorize_data_path}/test')\n",
    "    \n",
    "    pickle.dump({'config': vectorization_layer.get_config(),\n",
    "                 'weights': vectorization_layer.get_weights()}\n",
    "                , open(f'{vectorize_data_path}/layer.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5f65ee0-a4be-4d50-89dd-24fa0698f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_op = kfp.components.create_component_from_func(vectorize_data,\n",
    "                                                       output_component_file='vectorize_data_component.yaml',\n",
    "                                                       base_image=\"python:3.8\",\n",
    "                                                       packages_to_install=['tensorflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c82ffeee-49b7-4c69-8b31-1d3cd4378acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abc35eff-9ffa-439a-b436-b1450decc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_data(max_features: int, sequence_length: int, embedding_type: str, embedding_dim: int,\n",
    "                   vectorize_data_path: comp.InputPath(str()),\n",
    "                   embedding_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import zipfile\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load Vectorization Layer\n",
    "    from_disk = pickle.load(open(f'{vectorize_data_path}/layer.pkl', \"rb\"))\n",
    "    vectorization_layer = TextVectorization.from_config(from_disk['config'])\n",
    "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "    vectorization_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "    vectorization_layer.set_weights(from_disk['weights'])\n",
    "    \n",
    "    def choose_embeddings(max_features, embedding_dim, sequence_length, vectorization_layer, embedding_type):\n",
    "        \n",
    "        if embedding_type == \"glove\":\n",
    "            return Glove.glove(max_features, embedding_dim, sequence_length, vectorization_layer)\n",
    "        elif embedding_type == \"standard\":\n",
    "            return tf_standard(max_features, embedding_dim, sequence_length)\n",
    "\n",
    "\n",
    "    def tf_standard(max_features, embedding_dim, sequence_length):\n",
    "        \n",
    "        # A integer input for vocab indices.\n",
    "        inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "        # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "        # 'embedding_dim'\n",
    "        \n",
    "        embeddings = layers.Embedding(input_dim=max_features,\n",
    "                             output_dim=embedding_dim,\n",
    "                             input_length=sequence_length)(inputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs, embeddings)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def glove():\n",
    "        glove_dir = download_glove()\n",
    "\n",
    "        # A integer input for vocab indices.\n",
    "        inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "        # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "        # 'embedding_dim'.\n",
    "        embedding_matrix = load_glove_vectors(glove_dir, vectorization_layer, max_features, embedding_dim)\n",
    "        \n",
    "        embeddings = layers.Embedding(input_dim=max_features,\n",
    "                             output_dim=embedding_dim,\n",
    "                             input_length=sequence_length,\n",
    "                             trainable=False,\n",
    "                             weights=[embedding_matrix])(inputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs, embeddings)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def download_glove():\n",
    "        glove_path = load_data_path\n",
    "        glove_dir = os.path.join(glove_path, 'glove.840B.300d')\n",
    "        glove_file = os.path.join(glove_path, 'glove.840B.300d.tar.gz')\n",
    "\n",
    "        if not os.path.exists(glove_file):\n",
    "            url = \"https://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "\n",
    "            tf.keras.utils.get_file(\"glove.840B.300d\", url,\n",
    "                                    untar=True, cache_dir=glove_path,\n",
    "                                    cache_subdir='')\n",
    "\n",
    "            with zipfile.ZipFile(glove_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(glove_dir)\n",
    "\n",
    "        glove_file = os.path.join(glove_dir, 'glove.840B.300d.txt')\n",
    "\n",
    "        return glove_file\n",
    "\n",
    "\n",
    "    def load_glove_vectors(glove_file, vectorization_layer, max_features, embedding_dim):\n",
    "        voc = vectorization_layer.get_vocabulary()\n",
    "        word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "        \"\"\"\n",
    "        The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "        100-dimensional, 200-dimensional, 300-dimensional. We'll use the 300D ones.\n",
    "        Let's make a dict mapping words (strings) to their NumPy vector representation:\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings_index = {}\n",
    "        with open(glove_file, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split(' ')\n",
    "                word = values[0]\n",
    "                weights = np.asarray([float(val) for val in values[1:]])\n",
    "                embeddings_index[word] = weights\n",
    "\n",
    "        \"\"\"\n",
    "         Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "         `Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "         vector for the word of index `i` in our `vectorizer`'s vocabulary.\n",
    "         \"\"\"\n",
    "\n",
    "        embedding_dim = embedding_dim\n",
    "        if max_features is not None:\n",
    "            vocab_len = max_features\n",
    "        else:\n",
    "            vocab_len = len(word_index) + 1\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim))\n",
    "        oov_count = 0\n",
    "        oov_words = []\n",
    "        for word, idx in word_index.items():\n",
    "            if idx < vocab_len:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[idx] = embedding_vector\n",
    "                else:\n",
    "                    oov_count += 1\n",
    "                    oov_words.append(word)\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    \n",
    "    embedding_layer = choose_embeddings(max_features, embedding_dim, sequence_length, vectorization_layer, embedding_type)\n",
    "    \n",
    "    #creating the  embedding directory\n",
    "    os.makedirs(embedding_data_path, exist_ok = True)\n",
    "    \n",
    "    #save the layer as model\n",
    "    embedding_layer.save(f'{embedding_data_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1ff3ca9-5c33-4ccd-a996-21024ac2cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_op = kfp.components.create_component_from_func(embedding_data,\n",
    "                                                         output_component_file='embedding_data_component.yaml',\n",
    "                                                         base_image=\"python:3.8\",\n",
    "                                                         packages_to_install=['tensorflow', 'numpy', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97c7edf4-dbbc-49c2-a268-f75696a643c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dbbae3d6-21d2-4bc1-bf87-e5d0adebf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers: int, rec_units: int, dense_units: int, dropout: float,\n",
    "                max_features: int, embedding_dim: int, sequence_length: int,\n",
    "                vectorize_data_path: comp.InputPath(str),\n",
    "                embedding_data_path: comp.InputPath(str),\n",
    "                load_data_path: comp.InputPath(str()),\n",
    "                model_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    import zipfile\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "\n",
    "    from tensorflow.keras import layers\n",
    "    from keras.layers import SimpleRNN, LSTM, GRU, Bidirectional\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load Datasets\n",
    "    raw_test_ds = tf.data.Dataset.load(f'{load_data_path}/raw_test')\n",
    "    \n",
    "    train_ds = tf.data.Dataset.load(f'{vectorize_data_path}/train')\n",
    "    val_ds = tf.data.Dataset.load(f'{vectorize_data_path}/val')\n",
    "    test_ds = tf.data.Dataset.load(f'{vectorize_data_path}/test') \n",
    "    \n",
    "    # Load Embedding Layer as model\n",
    "    \n",
    "    embedding_layer = keras.models.load_model(f'{embedding_data_path}/model.h5')\n",
    "\n",
    "    # Create Model\n",
    "\n",
    "    x = layers.Dropout(dropout)(embedding_layer.layers[-1].output)\n",
    "\n",
    "    x = Bidirectional(GRU(units=rec_units, activation=\"relu\", return_sequences=True))(x)\n",
    "    x = Bidirectional(GRU(units=rec_units, activation=\"relu\", return_sequences=False))(x)\n",
    "\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=embedding_layer.inputs, outputs=predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "    \n",
    "    model.save(f'{model_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6ccf7643-cf54-400a-b7c0-0d165065b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_op = kfp.components.create_component_from_func(build_model,\n",
    "                                                           output_component_file='build_model_component.yaml',\n",
    "                                                           base_image=\"python:3.8\",\n",
    "                                                           packages_to_install=['tensorflow', 'pathlib', 'numpy', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "74af78df-b309-49d8-adb8-6d7cb6a47df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "49ff0071-a3d9-4078-9fb7-7f20d2234010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs: int, batch_size: int, es: bool,\n",
    "                model_path: comp.InputPath(str()),\n",
    "                vectorize_data_path: comp.InputPath(str()), \n",
    "                train_path: comp.OutputPath(str())):\n",
    "\n",
    "    import os\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    model = keras.models.load_model(f'{model_path}/model.h5')\n",
    "    \n",
    "    train_ds = tf.data.Dataset.load(f'{vectorize_data_path}/train')\n",
    "    val_ds = tf.data.Dataset.load(f'{vectorize_data_path}/val')\n",
    "    test_ds = tf.data.Dataset.load(f'{vectorize_data_path}/test') \n",
    "    \n",
    "    \"\"\"\n",
    "     ## Train the model\n",
    "    \"\"\"\n",
    "\n",
    "    if es:\n",
    "        es_callback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True)\n",
    "    else:\n",
    "        es_callback = None\n",
    "\n",
    "    # Fit the model using the train and test datasets.\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=es_callback, batch_size=batch_size)\n",
    "\n",
    "    \"\"\"\n",
    "    ## Evaluate the model on the test set\n",
    "    \"\"\"\n",
    "    model.evaluate(test_ds)\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(train_path, exist_ok = True)\n",
    "    \n",
    "    model.save(f'{train_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9643ab72-e444-46fc-9c2c-edb58e93b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = kfp.components.create_component_from_func(train_model,\n",
    "                                                     output_component_file='train_model_component.yaml',\n",
    "                                                     base_image=\"python:3.8\",\n",
    "                                                     packages_to_install=['tensorflow', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43e64e70-4b18-4b75-97a8-eca7d4014f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "063c0f30-8e26-45dd-a83f-c2f7bd12f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model(train_path: comp.InputPath(str()),\n",
    "                      final_model_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    model = keras.models.load_model(f'{train_path}/model.h5')\n",
    "    \n",
    "    # A string input\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "    # Turn strings into vocab indices\n",
    "    indices = vectorize_layer(inputs)\n",
    "    # Turn vocab indices into predictions\n",
    "    outputs = model(indices)\n",
    "\n",
    "    # Our end to end model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(final_model_path, exist_ok = True)\n",
    "    \n",
    "    #saving the model \n",
    "    model.save(f'{final_model_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6e4f1c9d-8412-4f0b-8c11-9b0ec1afc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_final_model_op = kfp.components.create_component_from_func(build_final_model,\n",
    "                                                                 output_component_file='build_final_model_component.yaml',\n",
    "                                                                 base_image=\"python:3.8\",\n",
    "                                                                 packages_to_install=['tensorflow', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "caa0c443-1664-4430-81a7-7e1565312ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "03252b9c-9705-4270-a448-6eceb3b32b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_model(final_model_path: comp.InputPath(str())):\n",
    "    \n",
    "    import numpy as np\n",
    "    import keras\n",
    "    \n",
    "    model = keras.models.load_model(f'{final_model_path}/model.h5')\n",
    "    \n",
    "    sample_text = 'Tesla is nice as shit'\n",
    "\n",
    "    sentiment = model.predict(np.array([sample_text]))\n",
    "\n",
    "    # If the prediction is >= 0.0, it is positive else it is negative.\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a91ad6d5-2ccc-44b0-bd03-274c067db5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_op = kfp.components.create_component_from_func(play_with_model,\n",
    "                                                    base_image=\"python:3.8\",\n",
    "                                                    packages_to_install=['tensorflow', 'numpy', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f9c07967-4baa-4490-8b12-f15c56c63a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='Trial pipeline',\n",
    "   description='An example pipeline that performs pd formation and plotting class distibution.'\n",
    ")\n",
    "def trial_pipeline(\n",
    "                   epochs:int, \n",
    "                   batch_size:int, \n",
    "                   es:bool,\n",
    "                   hidden_layers:int, \n",
    "                   rec_units:int, \n",
    "                   dense_units:int,\n",
    "                   dropout:float,\n",
    "                   max_features:int,\n",
    "                   embedding_type: str,\n",
    "                   embedding_dim: int,\n",
    "                   sequence_length: int,\n",
    "                   load_data_path: str,\n",
    "                   vectorize_data_path:str,\n",
    "                   embedding_data_path:str,\n",
    "                   model_path:str,\n",
    "                   train_path:str,\n",
    "                   final_model_path:str,\n",
    "                  ):\n",
    "    download_container = download_op()\n",
    "    vectorize_container = vectorize_op(max_features, embedding_type, embedding_dim, sequence_length, download_container.output)\n",
    "    embedding_container = embedding_op(max_features, sequence_length, embedding_type, embedding_dim, vectorize_container.output)\n",
    "    model_container = build_model_op(hidden_layers, rec_units, dense_units, dropout,  max_features, embedding_dim, sequence_length, vectorize_container.output, embedding_container.output, download_container.output)\n",
    "    trained_container = train_op(epochs, batch_size, es, model_container.output, vectorize_container.output)\n",
    "    final_container = build_final_model_op(trained_container.output)\n",
    "    play_container = play_op(final_container.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74d95b5e-5036-4a80-9dad-47bf0ef3b234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "embedding_type = \"standard\"\n",
    "embedding_dim = 300\n",
    "sequence_length = 500\n",
    "\n",
    "hidden_layers = 1\n",
    "rec_units = 128\n",
    "dense_units = 128\n",
    "dropout = 0.5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "es = True\n",
    "\n",
    "load_data_path = \"/mnt\"\n",
    "vectorize_data_path = \"vectorize_data\"\n",
    "embedding_data_path = \"embedding_data\"\n",
    "model_path = \"model\"\n",
    "train_path = \"train\"\n",
    "final_model_path = \"final_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e944a6a-112c-4481-bc59-57fab841a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/experiments/details/e38d1a73-1922-498c-b6ff-c666eae61717\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/runs/details/0e71f873-bc86-4fd2-9afd-912ab959e19b\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=0e71f873-bc86-4fd2-9afd-912ab959e19b)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USERNAME = \"user@example.com\"\n",
    "PASSWORD = \"changeme\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "HOST = 'http://istio-ingressgateway.istio-system.svc.cluster.local:80'\n",
    "\n",
    "session = requests.Session()\n",
    "response = session.get(HOST)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(response.url, headers=headers, data=data)\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    ")\n",
    "\n",
    "arguments = {\"epochs\":epochs,\n",
    "             \"batch_size\":batch_size,\n",
    "             \"es\":es,\n",
    "             \"hidden_layers\":hidden_layers,\n",
    "             \"rec_units\":rec_units,\n",
    "             \"dense_units\":dense_units,\n",
    "             \"dropout\":dropout,\n",
    "             \"max_features\":max_features,\n",
    "             \"embedding_type\":embedding_type,\n",
    "             \"embedding_dim\":embedding_dim,\n",
    "             \"sequence_length\":sequence_length,\n",
    "             \"vectorize_data_path\":vectorize_data_path,\n",
    "             \"embedding_data_path\":embedding_data_path,\n",
    "             \"load_data_path\":load_data_path,\n",
    "             \"model_path\":model_path,\n",
    "             \"train_path\":train_path,\n",
    "             \"final_model_path\":final_model_path,\n",
    "            }\n",
    "\n",
    "client.create_run_from_pipeline_func(pipeline_func=trial_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a2a57-c0eb-4a49-a443-2ee5231d32d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fe8cc-853c-41c6-a6bb-0841fe7ff465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
