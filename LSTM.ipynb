{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf761ad8-2a7b-4bf4-a825-1519accd25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fe5c5a-b4d8-4417-a288-d52495061fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import requests\n",
    "\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e39556-54ac-4434-9b77-8ee87fa4d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset, Preprocess Data, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adf477a-9807-4fda-9d38-90846a9c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_IMDB(load_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from pathlib import Path\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Set all paths\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dir = os.path.join(load_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "    \n",
    "    if not os.path.exists(load_data_path):\n",
    "        os.makedirs(load_data_path) \n",
    "    \n",
    "    \"\"\"\n",
    "    ## Load IMDB Data\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_file = os.path.join(load_data_path, 'aclImdb_v1.tar.gz')\n",
    "\n",
    "    if not os.path.exists(dataset_file):\n",
    "\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "        dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                            untar=True, cache_dir=load_data_path,\n",
    "                                            cache_subdir='')\n",
    "\n",
    "        \"\"\"\n",
    "        ## Remove unused directory\n",
    "        \"\"\"\n",
    "\n",
    "        remove_dir = os.path.join(train_dir, 'unsup')\n",
    "\n",
    "        if os.path.isdir(remove_dir):\n",
    "            shutil.rmtree(remove_dir)\n",
    "                \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4357565b-fff1-48c9-87d8-05db9a483e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_IMDB_op = kfp.components.create_component_from_func(download_data_IMDB,\n",
    "                                                             output_component_file='load_data_component.yaml',\n",
    "                                                             base_image=\"python:3.8\",\n",
    "                                                             packages_to_install=['tensorflow', 'pathlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73162844-afdb-4c64-9c68-46bcd323a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_FP(load_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    from pathlib import Path\n",
    "    from datasets import load_dataset, DownloadMode\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Set all paths\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dir = os.path.join(load_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    train_pos_dir = os.path.join(train_dir, 'pos')\n",
    "    train_neg_dir = os.path.join(train_dir, 'neg')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "    test_pos_dir = os.path.join(test_dir, 'pos')\n",
    "    test_neg_dir = os.path.join(test_dir, 'neg')\n",
    "    \n",
    "    dirs = [load_data_path, dataset_dir, train_dir, train_pos_dir, train_neg_dir, test_dir, test_pos_dir, test_neg_dir]\n",
    "    \n",
    "    for directory in dirs:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory) \n",
    "                \n",
    "    \"\"\"\n",
    "    ## Load Financial Phrasebank Data\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_file = os.path.join(load_data_path, 'financial_phrasebank.csv')\n",
    "    \n",
    "    def write_to_folder():\n",
    "        dataset = pd.read_csv(dataset_file)\n",
    "\n",
    "        pos_df = dataset[dataset['label'] == 2]\n",
    "        write_txt(pos_df, train_pos_dir, test_pos_dir)\n",
    "\n",
    "        neg_df = dataset[dataset['label'] == 0]\n",
    "        write_txt(neg_df, train_neg_dir, test_neg_dir)\n",
    "        \n",
    "    def write_txt(dataset, train_dir, test_dir):\n",
    "        i = 0\n",
    "\n",
    "        for row in dataset.values:\n",
    "            if i % 2 == 1:\n",
    "                filename = os.path.join(train_dir, 'FP' + str(i) + '.txt')\n",
    "                f = open(filename, 'w', encoding='utf-8')\n",
    "                f.write(row[0])\n",
    "                f.close()\n",
    "                i += 1\n",
    "            elif i % 2 == 0:\n",
    "                filename = os.path.join(test_dir, 'FP' + str(i) + '.txt')\n",
    "                f = open(filename, 'w', encoding='utf-8')\n",
    "                f.write(row[0])\n",
    "                f.close()\n",
    "                i += 1     \n",
    "    \n",
    "    if not os.path.exists(dataset_file):\n",
    "        dataset = load_dataset(path='financial_phrasebank',\n",
    "                               name='sentences_allagree',\n",
    "                               download_mode=DownloadMode.FORCE_REDOWNLOAD)\n",
    "\n",
    "        for split, data in dataset.items():\n",
    "            data.to_csv(dataset_file, index=None)\n",
    "        \n",
    "        write_to_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17eda872-1dd7-4a86-9d5c-6f6f2e8d0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_FP_op = kfp.components.create_component_from_func(download_data_FP,\n",
    "                                                            output_component_file='load_data_component.yaml',\n",
    "                                                            base_image=\"python:3.8\",\n",
    "                                                            packages_to_install=['pathlib', 'datasets', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bf8d3a-e55e-4b18-8b0b-b40c504c0458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7f7bf8-5869-41ad-a215-8fc2cee300eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(load_data_path_IMDB: comp.InputPath(str()),\n",
    "               load_data_path_FP: comp.InputPath(str()),\n",
    "               merge_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import shutil\n",
    "    import os \n",
    "    import stat\n",
    "    \n",
    "    # Merge Datasets\n",
    "\n",
    "    def copytree(src, dst, symlinks=False, ignore=None):\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "            shutil.copystat(src, dst)\n",
    "        lst = os.listdir(src)\n",
    "        if ignore:\n",
    "            excl = ignore(src, lst)\n",
    "            lst = [x for x in lst if x not in excl]\n",
    "        for item in lst:\n",
    "            s = os.path.join(src, item)\n",
    "            d = os.path.join(dst, item)\n",
    "            if symlinks and os.path.islink(s):\n",
    "                if os.path.lexists(d):\n",
    "                    os.remove(d)\n",
    "                os.symlink(os.readlink(s), d)\n",
    "                try:\n",
    "                    st = os.lstat(s)\n",
    "                    mode = stat.S_IMODE(st.st_mode)\n",
    "                    os.lchmod(d, mode)\n",
    "                except:\n",
    "                    pass  # lchmod not available\n",
    "            elif os.path.isdir(s):\n",
    "                copytree(s, d, symlinks, ignore)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "\n",
    "    os.makedirs(merge_data_path, exist_ok = True)            \n",
    "                \n",
    "    copytree(load_data_path_IMDB, merge_data_path)  \n",
    "    copytree(load_data_path_FP, merge_data_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5b70d9-cf3a-49cb-bf2b-e71a6ba4133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_op = kfp.components.create_component_from_func(merge_data,\n",
    "                                                     output_component_file='merge_data_component.yaml',\n",
    "                                                     base_image=\"python:3.8\",\n",
    "                                                     packages_to_install=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc414719-c66c-4ab4-a8ef-b8c0a3c487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4fea410-dc11-4b37-a054-e4b54c16c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(merge_data_path: comp.InputPath(str()),\n",
    "                    preprocess_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import contractions\n",
    "    import emoji\n",
    "    import nltk\n",
    "    import os\n",
    "    import re\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from string import punctuation\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from spellchecker import SpellChecker\n",
    "    from typing import List, Union\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    dataset_dir = os.path.join(merge_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "    \n",
    "    # Download data for preprocessing\n",
    "    def downloadNLTK():\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "    \n",
    "    downloadNLTK()\n",
    "    \n",
    "    # Preprocess\n",
    "    def preprocess(text):\n",
    "        text = replace_companies(text)\n",
    "        text = remove_html_tags(text)\n",
    "        text = replace_url(text)\n",
    "        text = replace_emojis(text)\n",
    "        text = replace_atUser(text)\n",
    "        text = replace_smiley(text)\n",
    "        text = replace_emojis(text)\n",
    "        text = remove_leetspeak(text)\n",
    "        text = check_spelling(text, lang='en')\n",
    "        text = replace_contractions(text)\n",
    "        text = remove_punct(text)\n",
    "        text = replace_numbers(text)\n",
    "        text = to_lower(text)\n",
    "        text = lemmatize(text)\n",
    "        text = clean_white_space(text)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(text):\n",
    "        return re.sub('<[^<]+?>', '', text)\n",
    "    \n",
    "    def replace_url(text):\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'url', text)\n",
    "        text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def replace_atUser(text):\n",
    "        text = re.sub('@[^\\s]+', 'atUser', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_leetspeak(text):\n",
    "        return re.sub(r\"[A-Za-z]+\\d+[A-Za-z]+|\\d+[A-Za-z]+\\d+|[A-Za-z]+\\d+|\\d+[A-Za-z]+\", '', text).strip()\n",
    "    \n",
    "    def replace_numbers(text):\n",
    "        return re.sub(r\"\\b\\d+\\b\", \"number\", text)\n",
    "    \n",
    "    def replace_contractions(text):\n",
    "        expanded_words = []\n",
    "        for word in text.split():\n",
    "            expanded_words.append(contractions.fix(word))\n",
    "\n",
    "        return ' '.join(expanded_words)\n",
    "    \n",
    "    def remove_punct(text):\n",
    "        return ''.join(c for c in text if c not in punctuation)\n",
    "    \n",
    "\n",
    "    def lemmatize(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        lemmatized_word = [lemmatizer.lemmatize(word) for sent in nltk.sent_tokenize(text) for word in\n",
    "                           nltk.word_tokenize(sent)]\n",
    "        return \" \".join(lemmatized_word)\n",
    "    \n",
    "    def check_spelling(input_text_or_list: Union[str, List[str]], lang='en'):\n",
    "        \"\"\" Check and correct spellings of the text list \"\"\"\n",
    "        if input_text_or_list is None or len(input_text_or_list) == 0:\n",
    "            return ''\n",
    "        spelling_checker = SpellChecker(language=lang, distance=1)\n",
    "        spelling_checker.word_frequency.load_words([\"Elon\", \"Musk\"])\n",
    "\n",
    "\n",
    "        if isinstance(input_text_or_list, str):\n",
    "            if not input_text_or_list.islower():\n",
    "                input_text_or_list = input_text_or_list.lower()\n",
    "            tokens = word_tokenize(input_text_or_list)\n",
    "        else:\n",
    "            tokens = [token.lower() for token in input_text_or_list if token is not None and len(token) > 0]\n",
    "        misspelled = spelling_checker.unknown(tokens)\n",
    "        for word in misspelled:\n",
    "            tokens[tokens.index(word)] = spelling_checker.correction(word)\n",
    "\n",
    "        return ' '.join(filter(lambda x: str(x) if x is not None else '', tokens)).strip()\n",
    "\n",
    "    def replace_emojis(text):\n",
    "        # Group by same meaning\n",
    "        text = re.sub('[\\U0001F550-\\U0001F567]', \" of the clock \", text)\n",
    "\n",
    "        # Replace emojis by their short text\n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "        # Remove everything between emoji\n",
    "        text = re.sub(\n",
    "            r\"(?<=:[a-zA-Z])(.*?)(?=:)\",\n",
    "            lambda g: \"{}\".format(re.sub(r\"[^a-zA-Z]\", \"\", g.group(1))),\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        # Remove : at the beginning and the end of an emoji\n",
    "        text = text.replace(\":\", \" \")\n",
    "\n",
    "        return text \n",
    "    \n",
    "    def replace_smiley(text):\n",
    "        \"\"\"\n",
    "        Remove smileys\n",
    "        Sources: https://de.wiktionary.org/wiki/Verzeichnis:International/Smileys\n",
    "                 https://en.wiktionary.org/wiki/Appendix:Emoticons\n",
    "        \"\"\"\n",
    "\n",
    "        SMILEYS = {\n",
    "            \":)\": \"smile\",\n",
    "            \":-)\": \"smile\",\n",
    "            \":^)\": \"\",\n",
    "            \":-]\": \"smile\",\n",
    "            \"=]\": \"smile\",\n",
    "            \":]\": \"smile\",\n",
    "            \":D\": \"\",\n",
    "            \":-D\": \"\",\n",
    "            \":))\": \"\",\n",
    "            \";-]\": \"\",\n",
    "            \";o)\": \"\",\n",
    "            \"¦)\": \"\",\n",
    "            \"=:)\": \"\",\n",
    "            \":9\": \"\",\n",
    "            \"c:\": \"\",\n",
    "            \":'D\": \"\",\n",
    "            \"xD\": \"\",\n",
    "            \"XD\": \"\",\n",
    "            \"B)\": \"\",\n",
    "            \"B-)\": \"\",\n",
    "            \"8)\": \"\",\n",
    "            \"8-)\": \"\",\n",
    "            \"=8)\": \"\",\n",
    "            \"=8^)\": \"\",\n",
    "            \"=B)\": \"\",\n",
    "            \"=B^)\": \"\",\n",
    "            \"~8D\": \"\",\n",
    "            \"y=)\": \"\",\n",
    "            \">:)\": \"\",\n",
    "            \">:D\": \"\",\n",
    "            \">:>\": \"\",\n",
    "            \">:[]\": \"\",\n",
    "            \"^_^\": \"\",\n",
    "            \"^-^\": \"\",\n",
    "            \"^.^\": \"\",\n",
    "            \"^,^\": \"\",\n",
    "            \"^^\": \"\",\n",
    "            \"^^'\": \"\",\n",
    "            \"^^°\": \"\",\n",
    "            \"^////^\": \"\",\n",
    "            \"^o^\": \"\",\n",
    "            \"^O^\": \"\",\n",
    "            \"^0^\": \"\",\n",
    "            \"\\o/\": \"\",\n",
    "            \"<o/\": \"\",\n",
    "            \"<(^.^)>\": \"\",\n",
    "            \"-^_^-\": \"\",\n",
    "            \"*(^_^)*\": \"\",\n",
    "            \"*0*\": \"\",\n",
    "            \"Ü\": \"\",\n",
    "            \"*~*\": \"\",\n",
    "            \":>\": \"\",\n",
    "            \":i\": \"\",\n",
    "            \"l:\": \"\",\n",
    "            \":(\": \"sad\",\n",
    "            \":c\": \"sad\",\n",
    "            \":[\": \"sad\",\n",
    "            \"=(\": \"sad\",\n",
    "            \"=[\": \"sad\",\n",
    "            \":'(\": \"\",\n",
    "            \":,(\": \"\",\n",
    "            \";(\": \"\",\n",
    "            \";_;\": \"\",\n",
    "            \"T.T\": \"\",\n",
    "            \"T_T\": \"\",\n",
    "            \"Q_Q\": \"\",\n",
    "            \":S\": \"\",\n",
    "            \":-/\": \"\",\n",
    "            \":/\": \"\",\n",
    "            \":-I\": \"\",\n",
    "            \">:(\": \"\",\n",
    "            \">:o\": \"\",\n",
    "            \">:O\": \"\",\n",
    "            \">:@\": \"\",\n",
    "            \"DX\": \"\",\n",
    "            \":-E3\": \"\",\n",
    "            \"x_X\": \"\",\n",
    "            \"X_x\": \"\",\n",
    "            \"x_x\": \"\",\n",
    "            \"x.X\": \"\",\n",
    "            \"X.x\": \"\",\n",
    "            \"x.x\": \"\",\n",
    "            \"°_°\": \"\",\n",
    "            \">.<\": \"\",\n",
    "            \">,<\": \"\",\n",
    "            \"-.-\": \"\",\n",
    "            \"-,-\": \"\",\n",
    "            \"-_-\": \"\",\n",
    "            \"._.\": \"\",\n",
    "            \"^_°'\": \"\",\n",
    "            \"^,°'\": \"\",\n",
    "            \"Oo\": \"\",\n",
    "            \"oO\": \"\",\n",
    "            \"O.o'\": \"\",\n",
    "            \"cO\": \"\",\n",
    "            \"ô_o\": \"\",\n",
    "            \"Ô_ô\": \"\",\n",
    "            \"D:\": \"\",\n",
    "            \"D8<\": \"\",\n",
    "            \"O_O\": \"surprised\",\n",
    "            \"Ò_Ó\": \"\",\n",
    "            \"U_U\": \"\",\n",
    "            \"v_v\": \"\",\n",
    "            \":<\": \"\",\n",
    "            \"m(\": \"\",\n",
    "            \"°^°\": \"\",\n",
    "            \"(@_@)\": \"\",\n",
    "            \";.;\": \"\",\n",
    "            \";)\": \"\",\n",
    "            \";-)\": \"\",\n",
    "            \"^.-\": \"\",\n",
    "            \":§\": \"\",\n",
    "            \";D\": \"\",\n",
    "            \";-D\": \"\",\n",
    "            \":P\": \"\",\n",
    "            \":p\": \"\",\n",
    "            \"c[=\": \"\",\n",
    "            \":p~~~~~~\": \"\",\n",
    "            \":-*\": \"kiss\",\n",
    "            \":*\": \"kiss\",\n",
    "            \";*\": \"\",\n",
    "            \":-x\": \"\",\n",
    "            \"C:\": \"\",\n",
    "            \":o\": \"\",\n",
    "            \":-o\": \"\",\n",
    "            \":O\": \"\",\n",
    "            \"0:-)\": \"\",\n",
    "            \"O:-)\": \"\",\n",
    "            \"3:)\": \"\",\n",
    "            \"3:D\": \"\",\n",
    "            \"-.-zZz\": \"\",\n",
    "            \"(o)_(o)\": \"\",\n",
    "            \"($)_($)\": \"\",\n",
    "            \"^_-\": \"\",\n",
    "            \"//.o\": \"\",\n",
    "            \"^w^\": \"\",\n",
    "            \"=^_^=\": \"\",\n",
    "            \"x3\": \"\",\n",
    "            \"*_*\": \"\",\n",
    "            \"#-)\": \"\",\n",
    "            \"`*,...ò_Ó...,*´\": \"\",\n",
    "            \":-{}\": \"\",\n",
    "            \":ö\": \"\",\n",
    "            \"û_û\": \"\",\n",
    "            \"Ö_Ö\": \"\",\n",
    "            \":o)\": \"\",\n",
    "            \"cB\": \"\",\n",
    "            \"BD\": \"\",\n",
    "            \"Y_\": \"\",\n",
    "            \":-€\": \"\",\n",
    "            \":3\": \"\",\n",
    "            \"x'DD\": \"\",\n",
    "            \"l/l\": \"\",\n",
    "            \":o)>\": \"\",\n",
    "            \"(_8(I)\": \"\",\n",
    "            \"//:=|\": \"\",\n",
    "            \"<3\": \"\",\n",
    "            \"</3\": \"\",\n",
    "            \"<'3\": \"\",\n",
    "            \"<°(((><\": \"\",\n",
    "            \"<°{{{><\": \"\",\n",
    "            \"<°++++<\": \"\",\n",
    "            \">)))°>\": \"\",\n",
    "            \"o=(====>\": \"\",\n",
    "            \"@>--}---\": \"rose\",\n",
    "            \"@>-`-,--\": \"rose\",\n",
    "            \"(_|::|_)\": \"\",\n",
    "            \"c(_)\": \"\",\n",
    "            \"[:|]\": \"\",\n",
    "            \"(°oo°)\": \"\",\n",
    "            \"(.)(.)\": \"\",\n",
    "            \"( . Y . )\": \"\",\n",
    "            \"( . )\": \"\",\n",
    "            \"| . |\": \"\",\n",
    "            \").(\": \"\",\n",
    "            \"(_i_)\": \"\",\n",
    "            \"( Y )\": \"\",\n",
    "            \"8===D\": \"penis\"\n",
    "        }\n",
    "        text = text.split()\n",
    "        reformed = [SMILEYS[word] if word in SMILEYS else word for word in text]\n",
    "        return \" \".join(reformed)\n",
    "    \n",
    "    def replace_companies(text):\n",
    "        COMPANIES = {\n",
    "            \"Apple\": \"company\",\n",
    "            \"Biohit\": \"company\",\n",
    "            \"Componenta\": \"company\",\n",
    "            \"Facebook\": \"company\",\n",
    "            \"Finnish Aktia Group\": \"company\",\n",
    "            \"Finnish Bank of +àland\": \"company\",\n",
    "            \"Finnlines\": \"company\",\n",
    "            \"Fiskars\": \"company\",\n",
    "            \"Google\": \"company\",\n",
    "            \"HELSINKI ( AFX )\": \"company\",\n",
    "            \"HKScan\": \"company\",\n",
    "            \"Kemira\": \"company\",\n",
    "            \"MegaFon\": \"company\",\n",
    "            \"Metso Minerals\": \"company\",\n",
    "            \"Microsoft\": \"company\",\n",
    "            \"Nokia Corp.\": \"company\",\n",
    "            \"Nordea Group\": \"company\",\n",
    "            \"Ponsse\": \"company\",\n",
    "            \"Ramirent\": \"company\",\n",
    "            \"Ruukki\": \"company\",\n",
    "            \"Sanoma Oyj HEL\": \"company\",\n",
    "            \"Talentum\": \"company\",\n",
    "            \"Teleste Oyj HEL\": \"company\",\n",
    "            \"TeliaSonera TLSN\": \"company\",\n",
    "            \"Tesla\": \"company\",\n",
    "            \"Tiimari\": \"company\",\n",
    "            \"Vaahto Group\": \"company\",\n",
    "        }\n",
    "        text = text.split()\n",
    "        reformed = [COMPANIES[word] if word in COMPANIES else word for word in text]\n",
    "        return \" \".join(reformed)\n",
    "    \n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def clean_white_space(text):\n",
    "        return re.sub(' +', ' ', text)\n",
    "    \n",
    "    # Change in all folders\n",
    "    def preprocess_all_folders(dataset_dir):\n",
    "\n",
    "        dataset_train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "        dataset_train_dir_pos = os.path.join(dataset_train_dir, 'pos')\n",
    "        files = os.listdir(dataset_train_dir_pos)\n",
    "        preprocess_all_files(dataset_train_dir_pos, files)\n",
    "\n",
    "        dataset_train_dir_neg = os.path.join(dataset_train_dir, 'neg')\n",
    "        files = os.listdir(dataset_train_dir_neg)\n",
    "        preprocess_all_files(dataset_train_dir_neg, files)\n",
    "\n",
    "        dataset_test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "        dataset_test_dir_pos = os.path.join(dataset_test_dir, 'pos')\n",
    "        files = os.listdir(dataset_test_dir_pos)\n",
    "        preprocess_all_files(dataset_test_dir_pos, files)\n",
    "\n",
    "        dataset_test_dir_neg = os.path.join(dataset_test_dir, 'neg')\n",
    "        files = os.listdir(dataset_test_dir_neg)\n",
    "        preprocess_all_files(dataset_test_dir_neg, files)\n",
    "\n",
    "\n",
    "    def preprocess_all_files(path, files):\n",
    "        for file in files:\n",
    "            newfile = os.path.join(path, file)\n",
    "            replaceAll(newfile)\n",
    "\n",
    "\n",
    "    def replaceAll(file_name):\n",
    "        with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "            text = file.read()  # read file into memory\n",
    "\n",
    "        text = preprocess(text)  # make replacements\n",
    "\n",
    "        with open(file_name, 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(text)  # rewrite the file\n",
    "\n",
    "    preprocess_all_folders(dataset_dir)        \n",
    "            \n",
    "    # Split Datasets\n",
    "    batch_size = 32\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                    train_dir,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    subset=\"training\",\n",
    "                    seed=1337\n",
    "    )\n",
    "    \n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                    train_dir,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    subset=\"validation\",\n",
    "                    seed=1337\n",
    "    )\n",
    "    \n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                     test_dir, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    os.makedirs(preprocess_data_path, exist_ok = True)\n",
    "    \n",
    "    tf.data.Dataset.save(raw_train_ds, f'{preprocess_data_path}/raw_train')\n",
    "    tf.data.Dataset.save(raw_val_ds, f'{preprocess_data_path}/raw_val')\n",
    "    tf.data.Dataset.save(raw_test_ds, f'{preprocess_data_path}/raw_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d743e0-7304-4a5c-9ffb-ea62f8c7806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = kfp.components.create_component_from_func(preprocess_data,\n",
    "                                                          output_component_file='preprocess_data_component.yaml',\n",
    "                                                          base_image=\"python:3.8\",\n",
    "                                                          packages_to_install=['tensorflow==2.10', 'nltk', 'contractions', 'emoji', 'typing', \n",
    "                                                                               'pyspellchecker', 'spacy', 'gensim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "676d01f4-ae21-4622-81d9-568486896435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c370f9-faf9-4250-82a2-223a16b3bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(max_features: int, embedding_type: str, embedding_dim: int, sequence_length: int,\n",
    "                   preprocess_data_path: comp.InputPath(str()),\n",
    "                   vectorize_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    \n",
    "    raw_train_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_train')\n",
    "    raw_val_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_val')\n",
    "    raw_test_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_test')\n",
    "\n",
    "    # Build Vectorization Layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "            standardize=None,\n",
    "            max_tokens=max_features,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length,\n",
    "    )\n",
    "\n",
    "    # Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "    # dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "    # datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "    # Let's make a text-only dataset (no labels):\n",
    "    text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "    # Let's call `adapt`:\n",
    "    vectorization_layer.adapt(text_ds)\n",
    "        \n",
    "    # Vectorize the data.  \n",
    "    def vectorize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        return  vectorization_layer(text), label\n",
    "\n",
    "    train_ds = raw_train_ds.map(vectorize_text)\n",
    "    val_ds = raw_val_ds.map(vectorize_text)\n",
    "    test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "    # Do async prefetching / buffering of the data for best performance on GPU.\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    #creating the vectorize directory\n",
    "    os.makedirs(vectorize_data_path, exist_ok = True)\n",
    "    \n",
    "    tf.data.Dataset.save(train_ds, f'{vectorize_data_path}/train')\n",
    "    tf.data.Dataset.save(val_ds, f'{vectorize_data_path}/val')\n",
    "    tf.data.Dataset.save(test_ds, f'{vectorize_data_path}/test')\n",
    "    \n",
    "    pickle.dump({'config': vectorization_layer.get_config(),\n",
    "                 'weights': vectorization_layer.get_weights()}\n",
    "                , open(f'{vectorize_data_path}/layer.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5f65ee0-a4be-4d50-89dd-24fa0698f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_op = kfp.components.create_component_from_func(vectorize_data,\n",
    "                                                       output_component_file='vectorize_data_component.yaml',\n",
    "                                                       base_image=\"python:3.8\",\n",
    "                                                       packages_to_install=['tensorflow==2.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c82ffeee-49b7-4c69-8b31-1d3cd4378acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc35eff-9ffa-439a-b436-b1450decc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_data(max_features: int, sequence_length: int, embedding_type: str, embedding_dim: int,\n",
    "                   vectorize_data_path: comp.InputPath(str()),\n",
    "                   embedding_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import zipfile\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load Vectorization Layer\n",
    "    from_disk = pickle.load(open(f'{vectorize_data_path}/layer.pkl', \"rb\"))\n",
    "    vectorization_layer = TextVectorization.from_config(from_disk['config'])\n",
    "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "    vectorization_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "    vectorization_layer.set_weights(from_disk['weights'])\n",
    "    \n",
    "    def choose_embeddings(max_features, embedding_dim, sequence_length, vectorization_layer, embedding_type):\n",
    "        \n",
    "        if embedding_type == \"glove\":\n",
    "            return glove(max_features, embedding_dim, sequence_length, vectorization_layer)\n",
    "        elif embedding_type == \"standard\":\n",
    "            return tf_standard(max_features, embedding_dim, sequence_length)\n",
    "\n",
    "\n",
    "    def tf_standard(max_features, embedding_dim, sequence_length):\n",
    "        \n",
    "        # A integer input for vocab indices.\n",
    "        inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "        # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "        # 'embedding_dim'\n",
    "        \n",
    "        embeddings = layers.Embedding(input_dim=max_features,\n",
    "                             output_dim=embedding_dim,\n",
    "                             input_length=sequence_length)(inputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs, embeddings)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def glove(max_features, embedding_dim, sequence_length, vectorization_layer):\n",
    "        glove_dir = download_glove()\n",
    "\n",
    "        # A integer input for vocab indices.\n",
    "        inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "        # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "        # 'embedding_dim'.\n",
    "        embedding_matrix = load_glove_vectors(glove_dir, vectorization_layer, max_features, embedding_dim)\n",
    "        \n",
    "        embeddings = layers.Embedding(input_dim=max_features,\n",
    "                             output_dim=embedding_dim,\n",
    "                             input_length=sequence_length,\n",
    "                             trainable=False,\n",
    "                             weights=[embedding_matrix])(inputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs, embeddings)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def download_glove():\n",
    "        #creating the  embedding directory\n",
    "        os.makedirs(embedding_data_path, exist_ok = True)\n",
    "        \n",
    "        glove_path = embedding_data_path\n",
    "        glove_dir = os.path.join(glove_path, 'glove.840B.300d')\n",
    "        glove_file = os.path.join(glove_path, 'glove.840B.300d.tar.gz')\n",
    "\n",
    "        if not os.path.exists(glove_file):\n",
    "            url = \"https://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "\n",
    "            tf.keras.utils.get_file(\"glove.840B.300d\", url,\n",
    "                                    untar=True, cache_dir=glove_path,\n",
    "                                    cache_subdir='')\n",
    "\n",
    "            with zipfile.ZipFile(glove_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(glove_dir)\n",
    "\n",
    "        glove_file = os.path.join(glove_dir, 'glove.840B.300d.txt')\n",
    "\n",
    "        return glove_file\n",
    "\n",
    "\n",
    "    def load_glove_vectors(glove_file, vectorization_layer, max_features, embedding_dim):\n",
    "        voc = vectorization_layer.get_vocabulary()\n",
    "        word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "        \"\"\"\n",
    "        The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
    "        100-dimensional, 200-dimensional, 300-dimensional. We'll use the 300D ones.\n",
    "        Let's make a dict mapping words (strings) to their NumPy vector representation:\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings_index = {}\n",
    "        with open(glove_file, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split(' ')\n",
    "                word = values[0]\n",
    "                weights = np.asarray([float(val) for val in values[1:]])\n",
    "                embeddings_index[word] = weights\n",
    "\n",
    "        \"\"\"\n",
    "         Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
    "         `Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
    "         vector for the word of index `i` in our `vectorizer`'s vocabulary.\n",
    "         \"\"\"\n",
    "\n",
    "        embedding_dim = embedding_dim\n",
    "        if max_features is not None:\n",
    "            vocab_len = max_features\n",
    "        else:\n",
    "            vocab_len = len(word_index) + 1\n",
    "        embedding_matrix = np.zeros((vocab_len, embedding_dim))\n",
    "        oov_count = 0\n",
    "        oov_words = []\n",
    "        for word, idx in word_index.items():\n",
    "            if idx < vocab_len:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[idx] = embedding_vector\n",
    "                else:\n",
    "                    oov_count += 1\n",
    "                    oov_words.append(word)\n",
    "\n",
    "        return embedding_matrix\n",
    "    \n",
    "    \n",
    "    embedding_layer = choose_embeddings(max_features, embedding_dim, sequence_length, vectorization_layer, embedding_type)\n",
    "    \n",
    "    #creating the  embedding directory\n",
    "    os.makedirs(embedding_data_path, exist_ok = True)\n",
    "    \n",
    "    #save the layer as model\n",
    "    embedding_layer.save(f'{embedding_data_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1ff3ca9-5c33-4ccd-a996-21024ac2cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_op = kfp.components.create_component_from_func(embedding_data,\n",
    "                                                         output_component_file='embedding_data_component.yaml',\n",
    "                                                         base_image=\"python:3.8\",\n",
    "                                                         packages_to_install=['tensorflow==2.10', 'numpy', 'keras==2.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c7edf4-dbbc-49c2-a268-f75696a643c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbbae3d6-21d2-4bc1-bf87-e5d0adebf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers: int, rec_units: int, dense_units: int, dropout: float,\n",
    "                max_features: int, embedding_dim: int, sequence_length: int,\n",
    "                learning_rate: float, epsilon: float, clipnorm: float,\n",
    "                vectorize_data_path: comp.InputPath(str),\n",
    "                embedding_data_path: comp.InputPath(str),\n",
    "                preprocess_data_path: comp.InputPath(str()),\n",
    "                model_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    import zipfile\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "\n",
    "    from tensorflow.keras import layers\n",
    "    from keras.layers import SimpleRNN, LSTM, GRU, Bidirectional\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load Datasets\n",
    "    raw_test_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_test')\n",
    "    \n",
    "    train_ds = tf.data.Dataset.load(f'{vectorize_data_path}/train')\n",
    "    val_ds = tf.data.Dataset.load(f'{vectorize_data_path}/val')\n",
    "    test_ds = tf.data.Dataset.load(f'{vectorize_data_path}/test') \n",
    "    \n",
    "    # Load Embedding Layer as model\n",
    "    \n",
    "    embedding_layer = keras.models.load_model(f'{embedding_data_path}/model.h5')\n",
    "\n",
    "    # Create Model\n",
    "\n",
    "    x = layers.Dropout(dropout)(embedding_layer.layers[-1].output)\n",
    "\n",
    "    if hidden_layers > 1:\n",
    "        for i in range(1, hidden_layers):\n",
    "            x = LSTM(units=self.rec_units, activation=\"relu\", return_sequences=True)(x)\n",
    "\n",
    "    x = LSTM(units=self.rec_units, activation=\"relu\")(x)\n",
    "\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=embedding_layer.inputs, outputs=predictions)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=clipnorm), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.metrics.BinaryAccuracy(),\n",
    "                           tf.keras.metrics.Precision(thresholds=0),\n",
    "                           tf.keras.metrics.Recall(thresholds=0)])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "    \n",
    "    model.save(f'{model_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ccf7643-cf54-400a-b7c0-0d165065b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_op = kfp.components.create_component_from_func(build_model,\n",
    "                                                           output_component_file='build_model_component.yaml',\n",
    "                                                           base_image=\"python:3.8\",\n",
    "                                                           packages_to_install=['tensorflow==2.10', 'pathlib', 'numpy', 'keras==2.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74af78df-b309-49d8-adb8-6d7cb6a47df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49ff0071-a3d9-4078-9fb7-7f20d2234010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs: int, batch_size: int, es: bool,\n",
    "                model_path: comp.InputPath(str()),\n",
    "                vectorize_data_path: comp.InputPath(str()), \n",
    "                train_path: comp.OutputPath(str())) -> NamedTuple(\n",
    "                'VisualizationOutput', [\n",
    "                    ('echo', 'string'), \n",
    "                    ('mlpipeline_ui_metadata', 'UI_metadata')\n",
    "                ]):\n",
    "\n",
    "    import os\n",
    "    import keras\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    model = keras.models.load_model(f'{model_path}/model.h5')\n",
    "    \n",
    "    train_ds = tf.data.Dataset.load(f'{vectorize_data_path}/train')\n",
    "    val_ds = tf.data.Dataset.load(f'{vectorize_data_path}/val')\n",
    "    test_ds = tf.data.Dataset.load(f'{vectorize_data_path}/test') \n",
    "    \n",
    "    \"\"\"\n",
    "     ## Train the model\n",
    "    \"\"\"\n",
    "\n",
    "    if es:\n",
    "        es_callback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True)\n",
    "    else:\n",
    "        es_callback = None\n",
    "\n",
    "    # Fit the model using the train and test datasets.\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=es_callback, batch_size=batch_size)\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(train_path, exist_ok = True)\n",
    "    \n",
    "    model.save(f'{train_path}/model.h5')\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Evaluate the model on the test set\n",
    "    \"\"\"\n",
    "    loss, binary_accuracy, precision, recall = model.evaluate(test_ds)\n",
    "    f1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    \n",
    "    # visualization of the results\n",
    "    visualization = f\"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <table>\n",
    "                <thead>\n",
    "                  <tr>\n",
    "                    <th>Binary Accuracy</th>\n",
    "                    <th>\"{binary_accuracy}\"</th>\n",
    "                  </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                  <tr>\n",
    "                    <td>Precision<br></td>\n",
    "                    <td>\"{precision}\"</td>\n",
    "                  </tr>\n",
    "                  <tr>\n",
    "                    <td>Recall<br></td>\n",
    "                    <td>\"{recall}\"</td>\n",
    "                  </tr>\n",
    "                  <tr>\n",
    "                    <td>F1 Score<br></td>\n",
    "                    <td>\"{f1_score}\"</td>\n",
    "                  </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'storage': 'inline',\n",
    "            'source': visualization,\n",
    "            'type': 'web-app',\n",
    "        }]\n",
    "    }\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('VisualizationOutput', ['echo', 'mlpipeline_ui_metadata'])\n",
    "    return output('Visualization', json.dumps(metadata))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9643ab72-e444-46fc-9c2c-edb58e93b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = kfp.components.create_component_from_func(train_model,\n",
    "                                                     output_component_file='train_model_component.yaml',\n",
    "                                                     base_image=\"python:3.8\",\n",
    "                                                     packages_to_install=['tensorflow==2.10', 'keras==2.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43e64e70-4b18-4b75-97a8-eca7d4014f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "063c0f30-8e26-45dd-a83f-c2f7bd12f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_model(learning_rate: float, epsilon: float, clipnorm: float,\n",
    "                      preprocess_data_path: comp.InputPath(str()),\n",
    "                      vectorize_data_path: comp.InputPath(str()), \n",
    "                      train_path: comp.InputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "    \n",
    "    raw_test_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_test')\n",
    "    \n",
    "    # Load Vectorization Layer\n",
    "    from_disk = pickle.load(open(f'{vectorize_data_path}/layer.pkl', \"rb\"))\n",
    "    vectorization_layer = TextVectorization.from_config(from_disk['config'])\n",
    "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "    vectorization_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "    vectorization_layer.set_weights(from_disk['weights'])\n",
    "    \n",
    "    model = keras.models.load_model(f'{train_path}/model.h5')\n",
    "    \n",
    "    # A string input\n",
    "    inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "    # Turn strings into vocab indices\n",
    "    indices = vectorization_layer(inputs)\n",
    "    # Turn vocab indices into predictions\n",
    "    outputs = model(indices)\n",
    "\n",
    "    # Our end to end model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=clipnorm), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.metrics.BinaryAccuracy(),\n",
    "                           tf.keras.metrics.Precision(thresholds=0),\n",
    "                           tf.keras.metrics.Recall(thresholds=0)])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(f'/mnt', exist_ok = True)\n",
    "    \n",
    "    #saving the model \n",
    "    with open(f'/mnt/model.pickle', 'wb') as file:\n",
    "        pickle.dump(model, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e4f1c9d-8412-4f0b-8c11-9b0ec1afc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_final_model_op = kfp.components.create_component_from_func(build_final_model,\n",
    "                                                                 output_component_file='build_final_model_component.yaml',\n",
    "                                                                 base_image=\"python:3.8\",\n",
    "                                                                 packages_to_install=['tensorflow==2.10', 'keras==2.10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9c07967-4baa-4490-8b12-f15c56c63a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='LSTM pipeline',\n",
    "   description='An example pipeline that performs for a sentiment model'\n",
    ")\n",
    "def lstm_pipeline(\n",
    "                   epochs:int, \n",
    "                   batch_size:int, \n",
    "                   es:bool,\n",
    "                   hidden_layers:int, \n",
    "                   rec_units:int, \n",
    "                   dense_units:int,\n",
    "                   dropout:float,\n",
    "                   learning_rate: float,\n",
    "                   epsilon: float,\n",
    "                   clipnorm: float,\n",
    "                   max_features:int,\n",
    "                   embedding_type: str,\n",
    "                   embedding_dim: int,\n",
    "                   sequence_length: int,\n",
    "                   load_data_path: str,\n",
    "                   merge_data_path: str,\n",
    "                   preprocess_data_path: str,\n",
    "                   vectorize_data_path:str,\n",
    "                   embedding_data_path:str,\n",
    "                   model_path:str,\n",
    "                   train_path:str,\n",
    "                  ):\n",
    "    \n",
    "    download_IMDB_container = download_IMDB_op()\n",
    "    download_FP_container = download_FP_op()\n",
    "    merge_container = merge_op(download_IMDB_container.output, download_FP_container.output).after(download_IMDB_container).after(download_FP_container)\n",
    "    preprocess_container = preprocess_op(merge_container.output)\n",
    "    vectorize_container = vectorize_op(max_features, embedding_type, embedding_dim, sequence_length, preprocess_container.output)\n",
    "    embedding_container = embedding_op(max_features, sequence_length, embedding_type, embedding_dim, vectorize_container.output)\n",
    "    model_container = build_model_op(hidden_layers, rec_units, dense_units, dropout,  max_features, embedding_dim, sequence_length, learning_rate, epsilon, clipnorm, vectorize_container.output, embedding_container.output, preprocess_container.output)\n",
    "    trained_container = train_op(epochs, batch_size, es, model_container.output, vectorize_container.output)\n",
    "    final_container = build_final_model_op(learning_rate, epsilon, clipnorm, preprocess_container.output, vectorize_container.output, trained_container.outputs[\"train\"]).add_pvolumes({\"/mnt\": dsl.PipelineVolume(pvc=\"model-volume\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74d95b5e-5036-4a80-9dad-47bf0ef3b234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features = 25000\n",
    "embedding_type = \"glove\"\n",
    "embedding_dim = 300\n",
    "sequence_length = 500\n",
    "\n",
    "hidden_layers = 1\n",
    "rec_units = 256\n",
    "dense_units = 256\n",
    "dropout = 0.5\n",
    "\n",
    "learning_rate=3e-5 \n",
    "epsilon=1e-08 \n",
    "clipnorm=1.0\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "es = True\n",
    "\n",
    "load_data_path = \"/mnt\"\n",
    "merge_data_path = \"merge\"\n",
    "preprocess_data_path = \"preprocess_data\"\n",
    "vectorize_data_path = \"vectorize_data\"\n",
    "embedding_data_path = \"embedding_data\"\n",
    "model_path = \"model\"\n",
    "train_path = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e944a6a-112c-4481-bc59-57fab841a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/experiments/details/75bd83b5-5582-4563-ba33-68668b8d272a\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/runs/details/907939bd-9fc7-4f40-9720-212bb374ce20\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=907939bd-9fc7-4f40-9720-212bb374ce20)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USERNAME = \"user@example.com\"\n",
    "PASSWORD = \"hsb1234#\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "HOST = 'http://istio-ingressgateway.istio-system.svc.cluster.local:80'\n",
    "\n",
    "session = requests.Session()\n",
    "response = session.get(HOST)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(response.url, headers=headers, data=data)\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    ")\n",
    "\n",
    "arguments = {\"epochs\":epochs,\n",
    "             \"batch_size\":batch_size,\n",
    "             \"es\":es,\n",
    "             \"hidden_layers\":hidden_layers,\n",
    "             \"rec_units\":rec_units,\n",
    "             \"dense_units\":dense_units,\n",
    "             \"dropout\":dropout,\n",
    "             \"max_features\":max_features,\n",
    "             \"learning_rate\": learning_rate,\n",
    "             \"epsilon\": epsilon,\n",
    "             \"clipnorm\": clipnorm,\n",
    "             \"embedding_type\":embedding_type,\n",
    "             \"embedding_dim\":embedding_dim,\n",
    "             \"sequence_length\":sequence_length,\n",
    "             \"vectorize_data_path\":vectorize_data_path,\n",
    "             \"embedding_data_path\":embedding_data_path,\n",
    "             \"load_data_path\":load_data_path,\n",
    "             \"merge_data_path\":merge_data_path,\n",
    "             \"preprocess_data_path\":preprocess_data_path,\n",
    "             \"model_path\":model_path,\n",
    "             \"train_path\":train_path,\n",
    "            }\n",
    "\n",
    "client.create_run_from_pipeline_func(pipeline_func=lstm_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73773893-d2af-45b9-b181-82adc812679b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b02b7a-2499-4676-8792-b8293048d66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
