{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf761ad8-2a7b-4bf4-a825-1519accd25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fe5c5a-b4d8-4417-a288-d52495061fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import requests\n",
    "\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e39556-54ac-4434-9b77-8ee87fa4d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Dataset, Preprocess Data, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2adf477a-9807-4fda-9d38-90846a9c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_IMDB(load_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from pathlib import Path\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Set all paths\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dir = os.path.join(load_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "    \n",
    "    if not os.path.exists(load_data_path):\n",
    "        os.makedirs(load_data_path) \n",
    "    \n",
    "    \"\"\"\n",
    "    ## Load IMDB Data\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_file = os.path.join(load_data_path, 'aclImdb_v1.tar.gz')\n",
    "\n",
    "    if not os.path.exists(dataset_file):\n",
    "\n",
    "        url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "        dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                            untar=True, cache_dir=load_data_path,\n",
    "                                            cache_subdir='')\n",
    "\n",
    "        \"\"\"\n",
    "        ## Remove unused directory\n",
    "        \"\"\"\n",
    "\n",
    "        remove_dir = os.path.join(train_dir, 'unsup')\n",
    "\n",
    "        if os.path.isdir(remove_dir):\n",
    "            shutil.rmtree(remove_dir)\n",
    "                \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4357565b-fff1-48c9-87d8-05db9a483e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_IMDB_op = kfp.components.create_component_from_func(download_data_IMDB,\n",
    "                                                             output_component_file='load_data_component.yaml',\n",
    "                                                             base_image=\"python:3.8\",\n",
    "                                                             packages_to_install=['tensorflow', 'pathlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73162844-afdb-4c64-9c68-46bcd323a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_FP(load_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    from pathlib import Path\n",
    "    from datasets import load_dataset, DownloadMode\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Set all paths\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_dir = os.path.join(load_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    train_pos_dir = os.path.join(train_dir, 'pos')\n",
    "    train_neg_dir = os.path.join(train_dir, 'neg')\n",
    "    test_dir = os.path.join(dataset_dir, 'test')\n",
    "    test_pos_dir = os.path.join(test_dir, 'pos')\n",
    "    test_neg_dir = os.path.join(test_dir, 'neg')\n",
    "    \n",
    "    dirs = [load_data_path, dataset_dir, train_dir, train_pos_dir, train_neg_dir, test_dir, test_pos_dir, test_neg_dir]\n",
    "    \n",
    "    for directory in dirs:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory) \n",
    "                \n",
    "    \"\"\"\n",
    "    ## Load Financial Phrasebank Data\n",
    "    \"\"\"\n",
    "    \n",
    "    def write_to_folder():\n",
    "        dataset = pd.read_csv(dataset_file)\n",
    "\n",
    "        pos_df = dataset[dataset['label'] == 2]\n",
    "        write_txt(pos_df, train_pos_dir, test_pos_dir)\n",
    "\n",
    "        neg_df = dataset[dataset['label'] == 0]\n",
    "        write_txt(neg_df, train_neg_dir, test_neg_dir)\n",
    "        \n",
    "    def write_txt(dataset, train_dir, test_dir):\n",
    "        i = 0\n",
    "\n",
    "        for row in dataset.values:\n",
    "            if i % 2 == 1:\n",
    "                filename = os.path.join(train_dir, 'FP' + str(i) + '.txt')\n",
    "                f = open(filename, 'w', encoding='utf-8')\n",
    "                f.write(row[0])\n",
    "                f.close()\n",
    "                i += 1\n",
    "            elif i % 2 == 0:\n",
    "                filename = os.path.join(test_dir, 'FP' + str(i) + '.txt')\n",
    "                f = open(filename, 'w', encoding='utf-8')\n",
    "                f.write(row[0])\n",
    "                f.close()\n",
    "                i += 1     \n",
    "    \n",
    "    dataset_file = os.path.join(load_data_path, 'financial_phrasebank.csv')\n",
    "    \n",
    "    if not os.path.exists(dataset_file):\n",
    "        dataset = load_dataset(path='financial_phrasebank',\n",
    "                               name='sentences_allagree',\n",
    "                               download_mode=DownloadMode.FORCE_REDOWNLOAD)\n",
    "\n",
    "        for split, data in dataset.items():\n",
    "            data.to_csv(dataset_file, index=None)\n",
    "        \n",
    "        write_to_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17eda872-1dd7-4a86-9d5c-6f6f2e8d0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_FP_op = kfp.components.create_component_from_func(download_data_FP,\n",
    "                                                            output_component_file='load_data_component.yaml',\n",
    "                                                            base_image=\"python:3.8\",\n",
    "                                                            packages_to_install=['pathlib', 'datasets', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc414719-c66c-4ab4-a8ef-b8c0a3c487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e4fea410-dc11-4b37-a054-e4b54c16c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch_size: int,\n",
    "                    load_data_path_IMDB: comp.InputPath(str()),\n",
    "                    load_data_path_FP: comp.InputPath(str()),\n",
    "                    preprocess_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import shutil\n",
    "    import os \n",
    "    import stat\n",
    "    \n",
    "    # Merge Datasets\n",
    "\n",
    "    def copytree(src, dst, symlinks=False, ignore=None):\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "            shutil.copystat(src, dst)\n",
    "        lst = os.listdir(src)\n",
    "        if ignore:\n",
    "            excl = ignore(src, lst)\n",
    "            lst = [x for x in lst if x not in excl]\n",
    "        for item in lst:\n",
    "            s = os.path.join(src, item)\n",
    "            d = os.path.join(dst, item)\n",
    "            if symlinks and os.path.islink(s):\n",
    "                if os.path.lexists(d):\n",
    "                    os.remove(d)\n",
    "                os.symlink(os.readlink(s), d)\n",
    "                try:\n",
    "                    st = os.lstat(s)\n",
    "                    mode = stat.S_IMODE(st.st_mode)\n",
    "                    os.lchmod(d, mode)\n",
    "                except:\n",
    "                    pass  # lchmod not available\n",
    "            elif os.path.isdir(s):\n",
    "                copytree(s, d, symlinks, ignore)\n",
    "            else:\n",
    "                shutil.copy2(s, d)\n",
    "\n",
    "\n",
    "    os.makedirs(preprocess_data_path, exist_ok = True)            \n",
    "                \n",
    "    copytree(load_data_path_IMDB, preprocess_data_path)  \n",
    "    copytree(load_data_path_FP, preprocess_data_path) \n",
    "    \n",
    "    dataset_dir = os.path.join(preprocess_data_path, 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    test_dir = os.path.join(dataset_dir, 'test') \n",
    "    \n",
    "    # Split Datasets\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                    train_dir,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    subset=\"training\",\n",
    "                    seed=1337\n",
    "    )\n",
    "    \n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                    train_dir,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    subset=\"validation\",\n",
    "                    seed=1337\n",
    "    )\n",
    "    \n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "                     test_dir, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    tf.data.Dataset.save(raw_train_ds, f'{preprocess_data_path}/raw_train')\n",
    "    tf.data.Dataset.save(raw_val_ds, f'{preprocess_data_path}/raw_val')\n",
    "    tf.data.Dataset.save(raw_test_ds, f'{preprocess_data_path}/raw_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "03d743e0-7304-4a5c-9ffb-ea62f8c7806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = kfp.components.create_component_from_func(preprocess_data,\n",
    "                                                          output_component_file='preprocess_data_component.yaml',\n",
    "                                                          base_image=\"python:3.8\",\n",
    "                                                          packages_to_install=['tensorflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "676d01f4-ae21-4622-81d9-568486896435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0c370f9-faf9-4250-82a2-223a16b3bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(preprocess_data_path: comp.InputPath(str()),\n",
    "                   vectorize_data_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    #@title Choose a BERT model to fine-tune\n",
    "\n",
    "    bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "    map_name_to_handle = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/google/electra_small/2',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/google/electra_base/2',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "    }\n",
    "\n",
    "    map_model_to_preprocess = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    }\n",
    "\n",
    "    tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "    tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "    \n",
    "    os.makedirs(vectorize_data_path, exist_ok = True)\n",
    "    \n",
    "    pickle.dump(tfhub_handle_encoder, open(f'{vectorize_data_path}/tfhub_handle_encoder.pkl', \"wb\"))\n",
    "    pickle.dump(tfhub_handle_preprocess, open(f'{vectorize_data_path}/tfhub_handle_preprocess.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5f65ee0-a4be-4d50-89dd-24fa0698f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_op = kfp.components.create_component_from_func(vectorize_data,\n",
    "                                                       output_component_file='vectorize_data_component.yaml',\n",
    "                                                       base_image=\"python:3.8\",\n",
    "                                                       packages_to_install=['tensorflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "97c7edf4-dbbc-49c2-a268-f75696a643c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dbbae3d6-21d2-4bc1-bf87-e5d0adebf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(epochs: int,\n",
    "                vectorize_data_path: comp.InputPath(str),\n",
    "                preprocess_data_path: comp.InputPath(str()),\n",
    "                model_path: comp.OutputPath(str())):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text as text\n",
    "    import pickle\n",
    "\n",
    "    from pathlib import Path\n",
    "    from official.nlp import optimization  # to create AdamW optimizer\n",
    "    \n",
    "    # Load Datasets   \n",
    "    train_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_train')\n",
    "    val_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_val')\n",
    "    test_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_test') \n",
    "    \n",
    "    tfhub_handle_encoder = pickle.load(open(f'{vectorize_data_path}/tfhub_handle_encoder.pkl', \"rb\"))\n",
    "    tfhub_handle_preprocess = pickle.load(open(f'{vectorize_data_path}/tfhub_handle_preprocess.pkl', \"rb\"))\n",
    "    \n",
    "    def build_classifier_model():\n",
    "        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "        preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "        encoder_inputs = preprocessing_layer(text_input)\n",
    "        encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "        outputs = encoder(encoder_inputs)\n",
    "        net = outputs['pooled_output']\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "        \n",
    "        return tf.keras.Model(text_input, net)\n",
    "        \n",
    "    \n",
    "    model = build_classifier_model() \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "    \n",
    "    model.save(f'{model_path}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ccf7643-cf54-400a-b7c0-0d165065b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_op = kfp.components.create_component_from_func(build_model,\n",
    "                                                           output_component_file='build_model_component.yaml',\n",
    "                                                           base_image=\"python:3.8\",\n",
    "                                                           packages_to_install=['tensorflow', 'tensorflow-hub', 'tf-models-official', 'tensorflow-text', 'keras','pathlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74af78df-b309-49d8-adb8-6d7cb6a47df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "49ff0071-a3d9-4078-9fb7-7f20d2234010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs: int, batch_size: int, es: bool,\n",
    "                model_path: comp.InputPath(str()),\n",
    "                preprocess_data_path: comp.InputPath(str()), \n",
    "                train_path: comp.OutputPath(str())):\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text as text\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from official.nlp import optimization  # to create AdamW optimizer\n",
    "    \n",
    "    train_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_train')\n",
    "    val_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_val')\n",
    "    test_ds = tf.data.Dataset.load(f'{preprocess_data_path}/raw_test')    \n",
    "    \n",
    "    model = keras.models.load_model(f'{model_path}/model.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "    \n",
    "    \"\"\"\n",
    "     ## Train the model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(train_path, exist_ok = True)\n",
    "    \n",
    "    #saving the model \n",
    "    model.save(f'/mnt/model.h5', include_optimizer=False)\n",
    "    \n",
    "    # visualization of the results\n",
    "    score, acc = model.evaluate(test_ds)\n",
    "    \n",
    "    visualization = f\"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            Accuracy: \"{acc}\"\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'storage': 'inline',\n",
    "            'source': visualization,\n",
    "            'type': 'web-app',\n",
    "        }]\n",
    "    }\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('VisualizationOutput', ['echo', 'mlpipeline_ui_metadata'])\n",
    "    return output('Visualization', json.dumps(metadata))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9643ab72-e444-46fc-9c2c-edb58e93b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = kfp.components.create_component_from_func(train_model,\n",
    "                                                     output_component_file='train_model_component.yaml',\n",
    "                                                     base_image=\"python:3.8\",\n",
    "                                                     packages_to_install=['tensorflow', 'tensorflow-hub', 'tf-models-official', 'tensorflow-text', 'keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f9c07967-4baa-4490-8b12-f15c56c63a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='transformer pipeline',\n",
    "   description='An example pipeline that performs for a sentiment model'\n",
    ")\n",
    "def transformer_pipeline(\n",
    "                   epochs:int, \n",
    "                   batch_size:int, \n",
    "                   es:bool,\n",
    "                   load_data_path: str,\n",
    "                   preprocess_data_path: str,\n",
    "                   vectorize_data_path:str,\n",
    "                   embedding_data_path:str,\n",
    "                   model_path:str,\n",
    "                   train_path:str,\n",
    "                  ):\n",
    "    \n",
    "    download_IMDB_container = download_IMDB_op()\n",
    "    download_FP_container = download_FP_op()\n",
    "    preprocess_container = preprocess_op(batch_size, download_IMDB_container.output, download_FP_container.output).after(download_IMDB_container).after(download_FP_container)\n",
    "    vectorize_container = vectorize_op(preprocess_container.output)\n",
    "    model_container = build_model_op(epochs, vectorize_container.output, preprocess_container.output)\n",
    "    trained_container = train_op(epochs, batch_size, es, model_container.output, preprocess_container.output).add_pvolumes({\"/mnt\": dsl.PipelineVolume(pvc=\"model-volume\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "74d95b5e-5036-4a80-9dad-47bf0ef3b234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "es = True\n",
    "\n",
    "load_data_path = \"/mnt\"\n",
    "preprocess_data_path = \"preprocess_data\"\n",
    "vectorize_data_path = \"vectorize_data\"\n",
    "model_path = \"model\"\n",
    "train_path = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e944a6a-112c-4481-bc59-57fab841a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/experiments/details/82ef02a8-2f4a-463d-8f70-c48847599285\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://istio-ingressgateway.istio-system.svc.cluster.local:80/pipeline/#/runs/details/abbb228c-c8d7-461b-bfbc-d6470d112fad\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=abbb228c-c8d7-461b-bfbc-d6470d112fad)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USERNAME = \"user@example.com\"\n",
    "PASSWORD = \"hsb1234#\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "HOST = 'http://istio-ingressgateway.istio-system.svc.cluster.local:80'\n",
    "\n",
    "session = requests.Session()\n",
    "response = session.get(HOST)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "session.post(response.url, headers=headers, data=data)\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "\n",
    "client = kfp.Client(\n",
    "    host=f\"{HOST}/pipeline\",\n",
    "    cookies=f\"authservice_session={session_cookie}\",\n",
    ")\n",
    "\n",
    "arguments = {\"epochs\":epochs,\n",
    "             \"batch_size\":batch_size,\n",
    "             \"es\":es,\n",
    "             \"vectorize_data_path\":vectorize_data_path,\n",
    "             \"load_data_path\":load_data_path,\n",
    "             \"preprocess_data_path\":preprocess_data_path,\n",
    "             \"model_path\":model_path,\n",
    "             \"train_path\":train_path,\n",
    "            }\n",
    "\n",
    "client.create_run_from_pipeline_func(pipeline_func=transformer_pipeline, arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73773893-d2af-45b9-b181-82adc812679b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b02b7a-2499-4676-8792-b8293048d66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
